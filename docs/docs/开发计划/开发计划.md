# 开发计划

记录需要开发的内容以及需要完善的需求


| 序号 | 开始时间 | 结束时间 | 需求 |
|------|--------|--------|---------|
| 19 | - | - | 支持单机多GPU训练|
| 18 | 2020-09-09 | - | 基于MSRA开发中文Bert+CRF ner模型|
| 17 | 2020-08-21 | 2020-09-07 | 梯度剪裁 |
| 16 | 2020-07-12 | 2020-07-19 | 属性级情感分析 |
| 15 | 2020-07-07 | 2020-07-07 | cnn 文本编码器|
| 14 | 2020-07-06 | 2020-07-07| 增加label decoder|
| 13 | 2020-07-01|2020-07-04 | 条件随机场 CRF|
| 12 | 2020-06-26 | 2020-06-29| conll2003 bilstm|
| 11 | 2020-06-21 | 2020-06-25 | event detedtion without trigger|
| 10 | 2020-06-25 | 2020-06-25| 增加训练 config factory|
| 9 | 2020-06-23 |2020-06-25 | 加入预训练词向量 |
| 8 | 2020-06-09 | 2020-06-09 | 英文tokenizer |
| 7 | 2020-06-03 | 2020-06-04 | 增加 learning rate scheduler |
| 6 | 2020-06-01| 2020-06-02| 单机单卡 GPU 训练|
| 5 | 2020-05-30 | 2020-05-30 | 增加 Acc metric|
| 4| 2020-05-28 | 2020-05-29 | metric 跟踪器 |
| 3 | 2020-05-29 | 2020-06-01 | 恢复训练 - 模型从文件载入 |
| 2 | 2020-05-27 | 2020-05-28 | 恢复训练 - optimize 从文件载入 |
| 1 | 2020-05-26 | 2020-05-27 | 继续训练 - 词汇表从文件载入 |


## 基于MSRA开发中文Bert+CRF ner模型

数据集选择 MSRA, 模型使用 Bert + CRF.

## 梯度剪裁

主要在 RNN 模型中引发的梯度爆炸和消失。

解决方案，在 trainner 中增加梯度裁剪。

## 属性级情感分析 (acsa)
完成 ATAE 属性级情感分析

## cnn 文本编码器
对文本进行 cnn 编码，生成向量.

包括功能:
1. 设置多个 filter
2. 输出 `一个向量`

## 增加 label decoder

包括如下功能:
1. 对模型输出能够解码出最终用户需要的结果。这在 predict 的时候，会使用。
2. 增加 常用的 label decoder, 包括两类:
   * logits 解码成 label index
   * label index 解码成最终要的 label结果

基于上面的修改，metric 也需要重构， metric 的输入，不是 logits,
而是 label index, 这样对于 metric 来说，输入都是 index, 方便统一处理。
而 logits 转换成 index 是通过 label decoder 来完成的。

这样做的好处是，做到代码重用。

## 条件随机场 CRF

增加 CRF。包括如下功能:

1. CRF 原理解析文档
2. 转移矩阵的限制能力，比如: B-Person 后面只能接 "I-Person" 或者 "O" 的转移矩阵限制能力

在 CRF 中，包含两个能力，分别是: CRF 以及 Viterbi. 其中: CRF 是用来计算 Loss 的模型，
Viterbi 是用来 decode label的。Viterbi 会用到 CRF 中计算得到的转移矩阵 Transition Matrix.

那么，从这一点来看，easy text 的结构应需要增加一个 decode label 层，该层的主要功能是将
模型的输出 "logits" 解析成 "Label"。那么, Viterbi 就是 decode label 中的一个算法。
decode label 层的不但对 predict 起到直接作用，也会对 Metric 计算起到直接作用。Inference
层将 "logits"  转化成 label index.

关于代码: CRF 有很成熟的实作，这里直接使用 AllenNlp 中的实作代码，不再另行开发。

## conll2003 bilstm
完成基于 conll2003 的 bilstm 基础模型.

## event detedtion without trigger
基于easytext 完成 该论文的实现。  
具体参考 `docs.docs.event.event_detection_without_trigger`

## 增加训练 config factory

开发时间: 2020-06-25

训练时候的各种配置，构建一个 config factory 来创建 config.

### 解决方案

在 `easytext/trainer/config_factory.py`, 增加 `ConfigFactory` 基类
用来创建 config. 这是一个抽象类。

如果是从 json 文件中创建，重写 create; 也可以直接 返回配置的字典。
具体配置什么自主权完全交给使用者。


## 加入预训练词向量

开发时间: 2020-06-23

加入预训练词向量。

### 解决方案

本质上是给 Vocabulary 中的所有 token 赋予一个 embedding vec.

1. 设计 Pretrained Loader，用来将 pretrianed 变成 embedding 字典
2. 设计 PretrainedVocabulary 新的类，将 Vocabulary 以及 Pretrained Loader 作为成员
3. 在 PretrainedVocabulary 将 Vocabulary 中按 index 排序的 token
去获取他的 embedding vec, 获取不到的 赋值成 全0.
4. PretrainedVocabulary 的接口保持与 Vocabulary 一样。
5. `torch.nn.Embedding.from_pretrained` 来导入预训练的 embedding matrix,
但是注意 `freeze` 参数，这是来决定是否要 fine-tuning 参数来用的。
6. 关于在训练时候，有两种方式方式，fine-tuning 和 freeze. 解决方案是在
`OptimizerFactory` 创建优化器的时候，进行配置。

#### Freeze 模式

在 `OptimizerFactory` 需要将 embedding 参数  
`embedding.weight.requires_grad = False`, 同时在 optimizer 中，
将该参数剔除掉。其实在 optimizer 中不剔除也没关系，剔除是表示严谨。

#### Fine-tuning 模式

在 `OptimizerFactory` 需要将 embedding 参数  
`embedding.weight.requires_grad = True`, 同时在 optimizer 中，
针对其学习率设置一个较小的值，因为仅仅是微调，与其他不同。


## 英文tokenizer

开发时间: 2020-06-09

对英文进行 tokenizer

### 解决方案

英文忽略大小写进行 tokenizer 化。 这是最简单的方式，复杂的可以使用 Bert 的 word piece
tokenizer, 这种情况要注意，在序列标注的任务中，label 的 index 会有问题，需要重新进行处理。

## 增加 learning rate scheduler

开发时间: 2020-06-03

### 解决方案

设计 `LRSchedulerFactory` 生成 lr scheduler。在trainer save
和 load 的时候进行保存和载入


## 单机单卡 GPU 训练

开发时间: 2020-06-01

### 需求描述

使用单机单卡进行GPU训练。后续，会增加新的 Feature 包括单机多卡 以及 多机多卡。


### 解决方案

1. `Tensor.cuda(device_id)` 将 tensor 放置到 gpu中
2. `Model` 是需要设置到 gpu 中的
3. `输入到模型的数据也都需要放置到 gpu 中`，所以 `data.ModelInputs` 需要转化
到gpu.

## Metric 跟踪器 - Metric Tracker

开始时间: 2020-05-28

### 需求描述
在训练过程中，需要 将每一个 epoch 产生训练集以及验证的 metric记录记录下来，  
以及`early stopping`来使用，所以 metric 跟踪器非常有必要开发，进行指标跟踪.

具体功能包含:

1. 保存每一个 epoch 的训练集和验证集的指标, 包括 target 指标
2. 保存 best 指标
3. 保存当前 epoch 以及 best epoch.
4. early stopping 计算


## 基于 checkpoint 恢复训练 - Recovery

开始时间: 2020-05-26

该功能的要求是，当设置一个 num_epoch  没有运行完，中间被停止下来，再次继续运行
的能力。这种 case 在很多情况下都有存在。涉及到两个能力分别是保存 checkpoint 以及重新
载入 checkpoint.

特别注意的地方: 因为是重新训练，所以要将前面运行的状态全部保存下来，以便进行恢复训练,
这包括:

1. vocabuary - 词汇表需要保存，直接从文件中读入
2. 模型 - 模型状态
3. optimzer - 优化器，需要设置成与先前一样的参数
4. best metric - 用作 early stopping 使用
5. patient_count - 用作  early stopping, 这个参数其实不跟进也无所谓，不影响训练结果。
因为，因为如果不保存这个 paitent count, 那么就是从0开始计算 直到满足 patient，仅仅
是多了几次训练而已。所以这里不去保存这个参数。

相关参数:

1. `num_keep_last` - 保存最后多少个state, 除了保存 best model 以外，
如果 `num_epoch` 非常大，全部保存下来是非常消耗存储资源的，为了恢复训练仅仅保存最后 `num_keep_last`
2. 保存路径 - 保存在哪里需要设置


包含的功能: 1. 词汇表保存与载入 2. 保存 checkpoint 与 载入

checkpoint包含的项目: 1. 模型 2. optimizer 3. best metric， 用来计算最好的模型比较

### checkpoint 保存

1. 保存的方案是在每一个epoch结束都要保存
2. 查看 metric 是否是最好的, 如果是将该模型保存成 best。第1个epoch的模型作为best保存。

### 遗留问题

metric 以及 loss 是否要保存历史的？ 这样可





